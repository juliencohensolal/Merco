{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/jcs_dl/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import HTML, display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Definitions\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "HTML(\"<style>.container { width: 80% !important; }</style>\")\n",
    "%matplotlib inline\n",
    "warnings.simplefilter('ignore')\n",
    "N_JOBS = 18\n",
    "SEED = 420\n",
    "FOLDS = 10\n",
    "SHIFT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Definitions for ML models\n",
    "\n",
    "def fit_lasso(X_train, y_train, seed = SEED, verbose = False) :\n",
    "    la = LassoCV(\n",
    "        alphas = [0.001, 0.003, 0.006, 0.01, 0.03, 0.06], \n",
    "        n_jobs = N_JOBS, \n",
    "        random_state = seed, \n",
    "        #tol = 0.0005,\n",
    "        max_iter = 50000, \n",
    "        normalize  = True)\n",
    "    la.fit(X_train, y_train)\n",
    "    \n",
    "    alpha = la.alpha_\n",
    "    la = LassoCV(\n",
    "        alphas = [alpha * .7, alpha * .75, alpha * .8, alpha * .85, alpha * .9, \n",
    "                  alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3], \n",
    "        n_jobs = N_JOBS, \n",
    "        random_state = seed, \n",
    "        #tol = 0.0005,\n",
    "        max_iter = 50000, \n",
    "        normalize  = True)\n",
    "    la.fit(X_train, y_train)\n",
    "    if (verbose) :\n",
    "        print(\"LASSO Best alpha :\", la.alpha_)\n",
    "        print(\"LASSO iter :\", la.n_iter_)\n",
    "\n",
    "    return (la)\n",
    "\n",
    "\n",
    "def fit_extra_trees(X_train, y_train, seed = SEED, verbose = False) :\n",
    "    et = ExtraTreesRegressor(\n",
    "        n_estimators = 300,\n",
    "        criterion = \"mse\", \n",
    "        max_features = 0.6,\n",
    "        max_depth = 6,\n",
    "        min_samples_split = 10,\n",
    "        min_samples_leaf = 10, \n",
    "        #bootstrap = True, \n",
    "        n_jobs = N_JOBS,\n",
    "        random_state = seed)\n",
    "    et.fit(X_train, y_train)                           \n",
    "\n",
    "    return (et)\n",
    "\n",
    "\n",
    "def xgb_r2_score(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return(\"r2\", r2_score(labels, preds))\n",
    "\n",
    "\n",
    "def fit_xgboost(X_train, y_train, seed = SEED, verbose = True) :    \n",
    "    y_mean = np.mean(y_train)\n",
    "    xg_params = {\n",
    "        \"eta\" : 0.004,\n",
    "        \"max_depth\" : 3,\n",
    "        \"min_child_weight\" : 2,\n",
    "        \"subsample\" : 1.0,\n",
    "        \"colsample_bytree\" : 0.8,\n",
    "        \"colsample_bylevel\" : 0.6,\n",
    "        \"nthread\" : N_JOBS, \n",
    "        \"base_score\" : y_mean,\n",
    "        \"eval_metric\" : \"rmse\",\n",
    "        \"objective\" : \"reg:linear\",\n",
    "    }\n",
    "    \n",
    "    # XGBoost needs a validation set to find best iteration\n",
    "    X_testVal = X_train.sample(frac = (1 / FOLDS), random_state = seed)\n",
    "    X_trainVal = X_train.drop(X_testVal.index, axis = 0)\n",
    "    X_testVal_y = y_train.sample(frac = (1 / FOLDS), random_state = seed)\n",
    "    X_trainVal_y = y_train.drop(X_testVal_y.index, axis = 0)\n",
    "\n",
    "    # Find best iteration\n",
    "    xg_trainVal = xgb.DMatrix(X_trainVal, label = X_trainVal_y)\n",
    "    xg_testVal = xgb.DMatrix(X_testVal, label = X_testVal_y)   \n",
    "    watchlist = [(xg_trainVal, \"train\"), (xg_testVal, \"eval\")]        \n",
    "    xg = xgb.train(\n",
    "        params = xg_params,\n",
    "        dtrain = xg_trainVal,\n",
    "        evals = watchlist, \n",
    "        num_boost_round = 100000,\n",
    "        early_stopping_rounds = 100,\n",
    "        feval = xgb_r2_score, \n",
    "        maximize = True,\n",
    "        verbose_eval = False)\n",
    "\n",
    "    if (verbose) :\n",
    "        print(\"XGBOOST best_iteration : \" + str(xg.best_iteration))\n",
    "        print(\"XGBOOST best_iteration adjusted : \" + str(int(xg.best_iteration * (1 + (1 / FOLDS)))))\n",
    "    best_iteration = int(xg.best_iteration * (1 + (1 / FOLDS)))\n",
    "    #best_iteration = xg.best_iteration\n",
    "    \n",
    "    # Now fit on whole train set with (approximated) best iteration\n",
    "    xg_train = xgb.DMatrix(X_train, label = y_train)\n",
    "    xg = xgb.train(\n",
    "        params = xg_params, \n",
    "        dtrain = xg_train, \n",
    "        num_boost_round = best_iteration,\n",
    "        #num_boost_round = 1250,\n",
    "        feval = xgb_r2_score, \n",
    "        verbose_eval = False)\n",
    "    \n",
    "    return (xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fit_predict(model, X_train, y_train, X_test, seed = SEED) :\n",
    "    # Fit\n",
    "    if(model == \"la\") :\n",
    "        predictor = fit_lasso(X_train, y_train.y, seed = seed)\n",
    "    elif(model == \"et\") :\n",
    "        predictor = fit_extra_trees(X_train, y_train.y, seed = seed)\n",
    "    elif(model == \"xg\") :\n",
    "        predictor = fit_xgboost(X_train, y_train.y, seed = seed, verbose = True)\n",
    "\n",
    "    # Predict\n",
    "    if (model == \"xg\") :\n",
    "        xg_test = xgb.DMatrix(X_test)\n",
    "        preds_X_test = predictor.predict(xg_test)\n",
    "    else :\n",
    "        preds_X_test = predictor.predict(X_test)\n",
    "    \n",
    "    return(preds_X_test)\n",
    "\n",
    "\n",
    "def get_cv_score(model, X_train, X_train_y) :\n",
    "    oof_preds = pd.DataFrame()\n",
    "    oof_targets = pd.DataFrame()\n",
    "    kf = KFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "        print(\"---------\")\n",
    "        print(\"FOLD \" + str(i + 1))\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = X_train_y.iloc[train_index], X_train_y.iloc[test_index]\n",
    "        \n",
    "        # Fit and predict\n",
    "        preds_X_test_fold = fit_predict(model, X_train_fold, y_train_fold, X_test_fold, seed = (SEED * i))\n",
    "\n",
    "        # Assemble OOF predictions\n",
    "        oof_preds = pd.concat([pd.Series(oof_preds), pd.Series(preds_X_test_fold)], axis = 0)\n",
    "        oof_targets = pd.concat([pd.Series(oof_targets), pd.Series(y_test_fold.y)], axis = 0)\n",
    "        \n",
    "    # Compute error on concatenated OOF predictions\n",
    "    cv_score = r2_score(oof_targets, oof_preds)\n",
    "    #cv_score = r2_score(np.exp(oof_targets) - SHIFT, np.exp(oof_preds) - SHIFT)\n",
    "    print(\"***** Global OOF r2_score : \" + str(cv_score) + \" *****\")\n",
    "                    \n",
    "    return(cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Script</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4209, 392)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3788, 392)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(421, 392)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4209, 392)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get data\n",
    "train = pd.read_csv(\"clean_data/train.csv\")\n",
    "y = pd.read_csv(\"clean_data/y.csv\")\n",
    "display(train.shape)\n",
    "\n",
    "X_train = pd.read_csv(\"clean_data/X_train.csv\")\n",
    "X_train_y = pd.read_csv(\"clean_data/X_train_y.csv\")\n",
    "display(X_train.shape)\n",
    "\n",
    "X_test = pd.read_csv(\"clean_data/X_test.csv\")\n",
    "X_test_y = pd.read_csv(\"clean_data/X_test_y.csv\")\n",
    "display(X_test.shape)\n",
    "\n",
    "test = pd.read_csv(\"clean_data/test.csv\")\n",
    "test_ids = pd.read_csv(\"clean_data/test_ids.csv\", header = None)\n",
    "display(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "FOLD 1\n",
      "XGBOOST best_iteration : 1152\n",
      "XGBOOST best_iteration adjusted : 1267\n",
      "---------\n",
      "FOLD 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-288028e7101f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Get CV score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mget_cv_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m '''# Save OOF preds for ensembling\n",
      "\u001b[0;32m<ipython-input-3-b3521d6862c0>\u001b[0m in \u001b[0;36mget_cv_score\u001b[0;34m(model, X_train, X_train_y)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Fit and predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mpreds_X_test_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Assemble OOF predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b3521d6862c0>\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(model, X_train, y_train, X_test, seed)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_extra_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"xg\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_xgboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-70e2fc1b4aab>\u001b[0m in \u001b[0;36mfit_xgboost\u001b[0;34m(X_train, y_train, seed, verbose)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mfeval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_r2_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mmaximize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         verbose_eval = False)\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/jcs_dl/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/jcs_dl/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# check evaluation result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mbst_eval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst_eval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTRING_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbst_eval_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/jcs_dl/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36meval_set\u001b[0;34m(self, evals, iteration, feval)\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[%d]'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m                 \u001b[0mfeval_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeval_ret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-70e2fc1b4aab>\u001b[0m in \u001b[0;36mxgb_r2_score\u001b[0;34m(preds, dtrain)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mxgb_r2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"r2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/jcs_dl/lib/python3.5/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mavg_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mavg_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/jcs_dl/lib/python3.5/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \"\"\"\n\u001b[1;32m   1023\u001b[0m     \u001b[0mCompute\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mweighted\u001b[0m \u001b[0maverage\u001b[0m \u001b[0malong\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = \"xg\"\n",
    "\n",
    "# Get CV score\n",
    "get_cv_score(model, train, y)\n",
    "\n",
    "'''# Save OOF preds for ensembling\n",
    "preds_X_test = fit_predict(model, X_train, X_train_y, X_test)\n",
    "pd.Series(preds_X_test).to_csv(\"preds/\" + model + \"_preds_X_test.csv\", index = False)'''\n",
    "\n",
    "# Predict test set\n",
    "preds_test = fit_predict(model, train, y, test)\n",
    "pd.DataFrame({\"ID\": test_ids[0], \"y\": preds_test}).to_csv(\"preds/\" + model + \"_preds_test.csv\", index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''        # Compute score on this fold\n",
    "         \n",
    "        fold_score = r2_score(y_val.y, preds_val)\n",
    "        #fold_score = r2_score(np.exp(y_val.y) - SHIFT, np.exp(preds_val) - SHIFT)\n",
    "        print(\"Fold r2_score : \" + str(fold_score))\n",
    "                \n",
    "        # Predict on test set having learned just on this X_train\n",
    "        preds_test = predictor.predict(test)\n",
    "        all_preds += preds_test\n",
    "    \n",
    "    # Compte error on OOF predictions\n",
    "    cv_score = r2_score(oof_targets, oof_preds)\n",
    "    #cv_score = r2_score(np.exp(oof_targets) - SHIFT, np.exp(oof_preds) - SHIFT)\n",
    "    print(\"Global OOF r2_score : \" + str(cv_score))\n",
    "    \n",
    "    # METHOD 1\n",
    "    # Compute average of test set predictions\n",
    "    preds_avg = all_preds / FOLDS\n",
    "    \n",
    "    # METHOD 2\n",
    "    # Learn and predict on whole sets\n",
    "    if(model == \"la\") :\n",
    "        predictor = fit_lasso(train, y, seed = (SEED * i))\n",
    "    elif(model == \"et\") :\n",
    "        predictor = fit_extra_trees(train, y, seed = (SEED * i))\n",
    "    preds_simple = predictor.predict(test)\n",
    "    \n",
    "    # Save data from la validation set for blending purposes\n",
    "    X_val.to_csv(\"clean_data/\" + model + \"_X_val.csv\", index = False)\n",
    "    y_val.y.to_csv(\"clean_data/y_val.csv\", index = False)    \n",
    "    pd.Series(preds_val).to_csv(\"clean_data/\" + model + \"_preds_val.csv\", index = False)    \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
